{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fd5f9eb-4d6c-45be-9634-fe54f1276d3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "# Cell 1: install dependencies & set your Hugging Face API token\n",
    "pip install torch transformers tqdm jsonschema sentence-transformers faiss-cpu huggingface_hub\n",
    "export HUGGINGFACE_HUB_TOKEN=\"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\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34d35e90-a14f-4aed-80ee-48a8ef5e2eed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 2: Imports & Configuration\n",
    "import os, json, logging\n",
    "from pathlib import Path\n",
    "\n",
    "# ensure HF token is visible to 🤗 transformers\n",
    "os.environ[\"HUGGINGFACE_HUB_TOKEN\"] = \"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\"\n",
    "\n",
    "import faiss\n",
    "import jsonschema\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\n",
    "\n",
    "# ─── Config ───────────────────────────────────────────────────────────────\n",
    "MODEL_NAME   = \"meta-llama/Meta-Llama-3-70B-Instruct\"\n",
    "CORPUS_FILE  = Path(\"seating_corpus.txt\")      # one seed seating puzzle per line\n",
    "OUTPUT_FILE  = Path(\"dataset_seating.json\")    # <-- final JSON path\n",
    "BATCH_SIZE   = 100\n",
    "TOTAL        = 4000\n",
    "BATCHES      = TOTAL // BATCH_SIZE\n",
    "RETRIES      = 3\n",
    "TOP_K        = 5       # RAG retrieve per batch\n",
    "DEVICE       = 0       # GPU index\n",
    "\n",
    "# JSON‐schema to enforce each batch\n",
    "SCHEMA = {\n",
    "    \"type\":\"array\",\n",
    "    \"minItems\":BATCH_SIZE,\"maxItems\":BATCH_SIZE,\n",
    "    \"items\":{\n",
    "      \"type\":\"object\",\n",
    "      \"properties\":{\n",
    "        \"topic\":{\"const\":\"Seating Arrangements\"},\n",
    "        \"question\":{\"type\":\"string\"},\n",
    "        \"choices\":{\n",
    "          \"type\":\"array\",\"minItems\":4,\"maxItems\":4,\n",
    "          \"items\":{\"type\":\"string\"}\n",
    "        },\n",
    "        \"answer\":{\"type\":\"string\",\"pattern\":\"^[A-D]$\"},\n",
    "        \"explanation\":{\"type\":\"string\"}\n",
    "      },\n",
    "      \"required\":[\"topic\",\"question\",\"choices\",\"answer\",\"explanation\"],\n",
    "      \"additionalProperties\":False\n",
    "    }\n",
    "}\n",
    "\n",
    "SYSTEM_BASE = \"\"\"You are a question-generation engine. Output **only** a JSON array of\n",
    "100 distinct, high-quality “Seating Arrangements” puzzles in this schema:\n",
    "\n",
    "[\n",
    "  {{\n",
    "    \"topic\": \"Seating Arrangements\",\n",
    "    \"question\": \"...\",\n",
    "    \"choices\": [\"A) ...\",\"B) ...\",\"C) ...\",\"D) ...\"],\n",
    "    \"answer\": \"<A|B|C|D>\",\n",
    "    \"explanation\": \"<≤100‐word rationale>\"\n",
    "  }},\n",
    "  … 100 items total …\n",
    "]\n",
    "\n",
    "Constraints:\n",
    "- Exactly 100 objects, no extra keys.\n",
    "- Include both linear and circular seating scenarios.\n",
    "- Exclude pure permutation/combination formula questions.\n",
    "- Vary people, seat‐counts, and positional constraints.\n",
    "- Keep each block under ~100 tokens.\n",
    "- **Output nothing but the JSON array**.\n",
    "\"\"\"\n",
    "\n",
    "USER_INSTR = \"Generate 100 such questions now.\"\n",
    "\n",
    "logging.basicConfig(level=logging.INFO, format=\"%(asctime)s %(levelname)s:%(message)s\")\n",
    "logger = logging.getLogger()\n",
    "\n",
    "# ─── Build FAISS index ───────────────────────────────────────────────────────\n",
    "def build_index(corpus_path: Path):\n",
    "    lines = [L.strip() for L in corpus_path.read_text().splitlines() if L.strip()]\n",
    "    embedder = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "    embs = embedder.encode(lines, convert_to_numpy=True, show_progress_bar=True)\n",
    "    idx = faiss.IndexFlatL2(embs.shape[1]); idx.add(embs)\n",
    "    return idx, lines, embedder\n",
    "\n",
    "# ─── Init Llama + pipeline ─────────────────────────────────────────────────\n",
    "logger.info(f\"Loading model {MODEL_NAME}…\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "model     = AutoModelForCausalLM.from_pretrained(MODEL_NAME,\n",
    "                torch_dtype=torch.float16, device_map=\"auto\")\n",
    "generator = pipeline(\"text-generation\", model=model, tokenizer=tokenizer,\n",
    "                     device=DEVICE, return_full_text=False)\n",
    "\n",
    "# ─── Generate one batch ─────────────────────────────────────────────────────\n",
    "def generate_batch(i, idx, corpus, embd):\n",
    "    # retrieve RAG examples\n",
    "    q_emb = embd.encode([\"Seating Arrangements puzzle\"], convert_to_numpy=True)\n",
    "    _, ids = idx.search(q_emb, TOP_K)\n",
    "    exs = [corpus[j] for j in ids[0]]\n",
    "    rag = \"\\n\\nExamples:\\n\" + \"\\n\".join(f\"- {e}\" for e in exs) + \"\\n\\n\"\n",
    "    prompt = SYSTEM_BASE + rag + \"USER:\\n\" + USER_INSTR\n",
    "\n",
    "    for attempt in range(1, RETRIES+1):\n",
    "        logger.info(f\"Batch {i+1}/{BATCHES} attempt {attempt}\")\n",
    "        out = generator(prompt, max_new_tokens=4096,\n",
    "                        temperature=0.8, top_p=0.95,\n",
    "                        do_sample=True,\n",
    "                        pad_token_id=tokenizer.eos_token_id)[0][\"generated_text\"]\n",
    "        try:\n",
    "            js = out[out.index(\"[\"):out.rindex(\"]\")+1]\n",
    "            arr = json.loads(js)\n",
    "            jsonschema.validate(arr, SCHEMA)\n",
    "            if len(arr)==BATCH_SIZE: return arr\n",
    "        except Exception as e:\n",
    "            logger.warning(f\"  failed: {e}\")\n",
    "    raise RuntimeError(f\"Batch {i+1} failed after {RETRIES} retries\")\n",
    "\n",
    "# ─── Main Loop ────────────────────────────────────────────────────────────────\n",
    "def main():\n",
    "    # Build index and generate all questions\n",
    "    idx, corpus, embd = build_index(CORPUS_FILE)\n",
    "    all_qs = []\n",
    "    for i in tqdm(range(BATCHES), desc=\"Generating\"):\n",
    "        all_qs.extend(generate_batch(i, idx, corpus, embd))\n",
    "\n",
    "    # Deduplicate by question text\n",
    "    uniq = {q[\"question\"]: q for q in all_qs}\n",
    "    deduped = list(uniq.values())\n",
    "    if len(deduped) < TOTAL:\n",
    "        logger.warning(f\"Only {len(deduped)} unique after dedupe\")\n",
    "\n",
    "    # Prepare output directory\n",
    "    from pathlib import Path\n",
    "    DATASET_DIR = Path(\"/jupyter-tutorial/DATASET\")\n",
    "    DATASET_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "    # Define full dataset path\n",
    "    FULL_OUT = DATASET_DIR / \"seating_arrangements_full.json\"\n",
    "\n",
    "    # Save only the full dataset\n",
    "    with open(FULL_OUT, \"w\") as f:\n",
    "        json.dump(deduped, f, indent=2)\n",
    "\n",
    "    print(f\"\\n✅ Full dataset saved to: {FULL_OUT.resolve()}\")\n",
    "\n",
    "# Execute in Jupyter\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "446beb65-7de0-453b-ac6f-eb9f2dfbd5bc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ebabc65-ae1c-4585-b8d5-043765c72f33",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
